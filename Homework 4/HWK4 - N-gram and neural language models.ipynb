{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zz-PFMXxN3fi"
   },
   "source": [
    "<center><h1>CSCI 4140: Natural Language Processing</h1></center>\n",
    "<center><h1>CSCI/DASC 6040: Computational Analysis of Natural Languages</h1></center>\n",
    "\n",
    "<center><h6>Spring 2023</h6></center>\n",
    "<center><h6>Homework 4 - N-gram and neural language models</h6></center>\n",
    "<center><h6>Due Sunday, March 26, at 11:59 PM</h6></center>\n",
    "\n",
    "<center><font color='red'>Do not redistribute without the instructor’s written permission.</font></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TcbM1VSrN3fj"
   },
   "source": [
    "# Setup\n",
    "<font color='red'>Notes:\n",
    "\n",
    "- You must run the code for Q2 on a computer with GPU (running it on CPU will take much, much longer). [Google Colab](https://colab.research.google.com) is a good choice.\n",
    "    - If you're using Colab, make sure you upload the `wiki` files.\n",
    "    - If you're using other computer, update the path to `wiki` files (`fname = \"...`).\n",
    "- The neural language model may take up to 10 minutes to train, so **start early**!\n",
    "- The rest of the cells are designed so that you can run them in a few minutes of computation time. If it is taking longer than that, you probably have made a mistake in your code.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "X8bZDdek4mo2"
   },
   "outputs": [],
   "source": [
    "import torch, pickle, os, sys, random, time\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import nn, optim\n",
    "from collections import *\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU: NVIDIA GeForce RTX 3070 Ti\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")          # a CUDA device object\n",
    "    print('Using GPU:', torch.cuda.get_device_name())\n",
    "else:\n",
    "    device = torch.device(\"cpu\")           # a CPU device object\n",
    "    print('Using CPU')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OCiaiiPsN3fk"
   },
   "source": [
    "We'll start by loading the data. The WikiText language modeling dataset is a collection of tokens extracted from the set of verified Good and Featured articles on Wikipedia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Y6IW47fRN3fl"
   },
   "outputs": [],
   "source": [
    "data = {'test': '', 'train': '', 'valid': ''}\n",
    "\n",
    "for data_split in data:\n",
    "    fname = \"wiki.{}.tokens\".format(data_split)\n",
    "    with open(fname, 'r', encoding='utf8') as f_wiki:\n",
    "        data[data_split] = f_wiki.read().lower().split()\n",
    "\n",
    "vocab = list(set(data['train']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FzRJcts-xirF"
   },
   "source": [
    "Now have a look at the data by running this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "GLE1v7mRIKt-",
    "outputId": "c15174a4-bd2d-4e6a-f763-de0e7ffb0329"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train : ['=', 'valkyria', 'chronicles', 'iii', '=', 'senjō', 'no', 'valkyria', '3', ':'] ...\n",
      "dev : ['=', 'homarus', 'gammarus', '=', 'homarus', 'gammarus', ',', 'known', 'as', 'the'] ...\n",
      "test : ['=', 'robert', '<unk>', '=', 'robert', '<unk>', 'is', 'an', 'english', 'film'] ...\n",
      "first 10 words in vocab: ['stegosauria', 'diệm', 'frakes', 'onça', 'probes', 'rightful', 'tasked', 'ethiopia', 'apted', '970']\n"
     ]
    }
   ],
   "source": [
    "print('train : %s ...' % data['train'][:10])\n",
    "print('dev : %s ...' % data['valid'][:10])\n",
    "print('test : %s ...' % data['test'][:10])\n",
    "print('first 10 words in vocab: %s' % vocab[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3TZ91yDrl1Oq"
   },
   "source": [
    "# Q1. N-gram Language model (40pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qhEuT7w5ClUg"
   },
   "source": [
    "\n",
    "## Q1.1: Train N-gram language model (15pts)\n",
    "\n",
    "Complete the following `train_ngram_lm` function based on the following input/output specifications. If you've done it right, you should pass the tests in the cell below.\n",
    "\n",
    "*Input:*\n",
    "+ **data**: the data object created in the cell above that holds the tokenized Wikitext data\n",
    "+ **order**: the order of the model (i.e., the \"n\" in \"n-gram\" model). If order=3, we compute $p(w_2 | w_0, w_1)$.\n",
    "\n",
    "*Output:*\n",
    "+ **lm**: A dictionary where the key is the history and the value is a probability distribution over the next word computed using the maximum likelihood estimate from the training data. Importantly, this dictionary should include *backoff* probabilities as well; e.g., for order=4, we want to store $p(w_3 | w_0,w_1,w_2)$ as well as $p(w_3|w_1,w_2)$ and $p(w_3|w_2)$. \n",
    "\n",
    "Each key should be a single string where the words that form the history have been concatenated using spaces. Given a key, its corresponding value should be a dictionary where each word type in the vocabulary is associated with its probability of appearing after the key. For example, the entry for the history 'w1 w2' should look like:\n",
    "\n",
    "    \n",
    "    lm['w1 w2'] = {'w0': 0.001, 'w1' : 1e-6, 'w2' : 1e-6, 'w3': 0.003, ...}\n",
    "    \n",
    "In this example, we also want to store `lm['w2']` and `lm['']`, which contain the bigram and unigram distributions respectively.\n",
    "\n",
    "*Hint:* You might find the **defaultdict** and **Counter** classes in the **collections** module to be helpful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "S13ismv99-N1"
   },
   "outputs": [],
   "source": [
    "def train_ngram_lm(data, order=3):\n",
    "    \"\"\"\n",
    "        Train n-gram language model\n",
    "    \"\"\"\n",
    "    \n",
    "    # pad (order-1) special tokens to the left\n",
    "    # for the first token in the text\n",
    "    order -= 1\n",
    "    data = ['<S>'] * order + data # \n",
    "    lm = defaultdict(Counter)\n",
    "    for o in range(order + 1):\n",
    "        for i in range(len(data) - o):\n",
    "            history = ' '.join(data[i:i+o])\n",
    "            next_word = data[i+o]\n",
    "            lm[history][next_word] += 1\n",
    "\n",
    "    # Normalize counts to probabilities\n",
    "    for history, next_word_counts in lm.items():\n",
    "        total_count = sum(next_word_counts.values())\n",
    "        for next_word, count in next_word_counts.items():\n",
    "            lm[history][next_word] = count / total_count\n",
    "\n",
    "    return lm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "QK0QYLxd49x3",
    "outputId": "3913e2bf-50bd-45a3-fcdc-19bbc84e6d8c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checking empty history ...\n",
      "checking probability distributions ...\n",
      "checking lengths of histories ...\n",
      "checking word distribution values ...\n",
      "Congratulations, you passed the ngram check!\n"
     ]
    }
   ],
   "source": [
    "def test_ngram_lm():\n",
    "  \n",
    "    print('checking empty history ...')\n",
    "    lm1 = train_ngram_lm(data['train'], order=1)\n",
    "    assert '' in lm1, \"empty history should be in the language model!\"\n",
    "    \n",
    "    print('checking probability distributions ...')\n",
    "    lm2 = train_ngram_lm(data['train'], order=2)\n",
    "    sample = [sum(lm2[k].values()) for k in random.sample(list(lm2), 10)]\n",
    "    assert all([a > 0.999 and a < 1.001 for a in sample]), \"lm[history][word] should sum to 1!\"\n",
    "    \n",
    "    print('checking lengths of histories ...')\n",
    "    lm3 = train_ngram_lm(data['train'], order=3)\n",
    "    assert len(set([len(k.split()) for k in list(lm3)])) == 3, \"lm object should store histories of all sizes!\"\n",
    "    \n",
    "    print('checking word distribution values ...')\n",
    "    assert lm1['']['the'] < 0.064 and lm1['']['the'] > 0.062 and \\\n",
    "           lm2['the']['first'] < 0.017 and lm2['the']['first'] > 0.016 and \\\n",
    "           lm3['the first']['time'] < 0.106 and lm3['the first']['time'] > 0.105, \\\n",
    "           \"values do not match!\"\n",
    "    \n",
    "    print(\"Congratulations, you passed the ngram check!\")\n",
    "    \n",
    "test_ngram_lm()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EjKT7pwJE6WY"
   },
   "source": [
    "## Q1.2: Generate text from n-gram language model (10pts)\n",
    "\n",
    "Complete the following `generate_text` function based on these input/output requirements:\n",
    "\n",
    "*Input:*\n",
    "\n",
    "+ **lm**: the lm object is the dictionary you return from  the **train_ngram_lm** function\n",
    "+ **vocab**: vocab is a list of unique word types in the training set, already computed for you during data loading.\n",
    "+ **context**: the input context string that you want to condition your language model on, should be a space-separated string of tokens\n",
    "+ **order**: order of your language model (i.e., \"n\" in the \"n-gram\" model)\n",
    "+ **num_tok**: number of tokens to be generated following the input context\n",
    "\n",
    "\n",
    "*Output:*\n",
    "\n",
    "+ generated text, should be a space-separated string\n",
    "    \n",
    "*Hint:*\n",
    "\n",
    "After getting the next-word distribution given history, try using **[numpy.random.choice](https://numpy.org/doc/stable/reference/random/generated/numpy.random.choice.html)** to sample the next word from the distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "jx0BFoF1E5dF"
   },
   "outputs": [],
   "source": [
    "# generate text\n",
    "def generate_text(lm, vocab, context=\"he is the\", order=3, num_tok=25):\n",
    "    \n",
    "    # The goal is to generate new words following the context\n",
    "    # If context has more tokens than the order of lm, \n",
    "    # generate text that follows the last (order-1) tokens of the context\n",
    "    # and store it in the variable `history`\n",
    "    order -= 1\n",
    "    history = context.split()[-order:]\n",
    "    # `out` is the list of tokens of context\n",
    "    # you need to append the generated tokens to this list\n",
    "    out = context.split()\n",
    "        \n",
    "    for i in range(num_tok):\n",
    "        # Get the probability distribution over the next word given the history\n",
    "        if tuple(history) in lm:\n",
    "            dist = lm[tuple(history)]\n",
    "        else:\n",
    "            # If the history is not in the lm, we choose a random word from the vocabulary\n",
    "            dist = np.ones(len(vocab))/len(vocab)\n",
    "        # Sample the next word from the distribution\n",
    "        next_word = np.random.choice(vocab, p=dist)\n",
    "        # Append the next word to the output\n",
    "        out.append(next_word)\n",
    "        # Update the history by removing the first word and adding the next word\n",
    "        history = history[1:] + [next_word]\n",
    "\n",
    "    # Concatenate the tokens in the `out` list into a single string and return it\n",
    "    return ' '.join(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2qPPhLK3HF5L"
   },
   "source": [
    "Now try to generate some texts! Read the texts generated by ngram language model with different orders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "BSlNevanHIlM",
    "outputId": "0069e46d-6b9e-4f55-9135-a4718d751f67"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'he is the convoys llosa frightened 1215 temporal afrodisiac insects flour asia falkland jowell celebrities winter slept coal wasn youth veteran umbo training copulation mentioning honeymoon saturn lee'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "order = 1\n",
    "generate_text(train_ngram_lm(data['train'], order=order), vocab, context='he is the', order=order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "Ibmvkwl9HMzd",
    "outputId": "eb364d37-42ed-4d4f-b821-f3d4c5676266"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'he is the emigration patriarchal across atenism egitto interpretive balloon thorns scofield data sexes berengaria proving switzerland antics rightful jai disguised arcade reliably worthless abe pursuit sivaji war'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "order = 2\n",
    "generate_text(train_ngram_lm(data['train'], order=order), vocab, context='he is the', order=order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "BQNB3FqKHibm",
    "outputId": "aaf46a26-5776-419f-e3a9-f2700c65a623"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'he is the technology helix privy maniac spectrograph nectar denouncing superfluous idiosyncratic polite mycena pressured deterioration valentin posse 1619 cosmetic aralt mayfair opener sacred this mearns subset unemployed'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "order = 3\n",
    "generate_text(train_ngram_lm(data['train'], order=order), vocab, context='he is the', order=order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "eSO4l5z7HjGU",
    "outputId": "7b33d144-1b25-4d96-c338-4df0add847d0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'he is the istanbul 176 graduates heart stereotype supergiants affray glasgow cruel scaffidi reason hubbardton desserts ridot extreme peronism diva expresses squadron inisfallen liga lakota generosity grow francetić'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "order = 4\n",
    "generate_text(train_ngram_lm(data['train'], order=order), vocab, context='he is the', order=order)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YLovkCIrHy0H"
   },
   "source": [
    "## Q1.3 : Evaluate the models (15pts)\n",
    "Now let's evaluate the models quantitively using the intrinsic metric **perplexity**. \n",
    "\n",
    "Recall perplexity is the inverse probability of the test text\n",
    "$$\\text{PP}(w_1, \\dots, w_t) = P(w_1, \\dots, w_t)^{-\\frac{1}{T}}$$\n",
    "\n",
    "For an n-gram model, perplexity is computed by\n",
    "$$\\text{PP}(w_1, \\dots, w_t) = \\left[\\prod_{t=1}^T P(w_t|w_{t-1},\\ldots,w_{t-n+1})\\right]^{-\\frac{1}{T}}$$\n",
    "\n",
    "To address the numerical issue (underflow), we usually compute\n",
    "$$\\text{PP}(w_1, \\dots, w_t) = \\exp\\left(-\\frac{1}{T}\\sum_i \\log P(w_t|w_{t-1},\\ldots,w_{t-n+1})\\right)$$\n",
    "\n",
    "\n",
    "*Input:*\n",
    "\n",
    "+ **lm**: the language model you trained (the object you returned from the `train_ngram_lm` function)\n",
    "+ **data**: test data\n",
    "+ **vocab**: the list of unique word types in the training set\n",
    "+ **order**: order of the lm\n",
    "\n",
    "*Output:*\n",
    "\n",
    "+ the perplexity of test data\n",
    "\n",
    "*Hint:*\n",
    "\n",
    "+ If the history is not in the **lm** object, back-off to (n-1) order history to check if it is in **lm**. If no history can be found, just use `1/|V|` where `|V|` is the size of vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "FKi4AczgHj1t"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "def compute_perplexity(lm, data, vocab, order=3):\n",
    "    # pad according to order\n",
    "    order -= 1\n",
    "    data = ['<S>'] * order + data\n",
    "    log_sum = 0\n",
    "    N = len(data) - order\n",
    "    for i in range(N):\n",
    "        h, w = ' '.join(data[i: i+order]), data[i+order]\n",
    "        # if h not in lm, back-off to n-1 gram and look up again\n",
    "        while order > 0 and h not in lm:\n",
    "            order -= 1\n",
    "            h = ' '.join(data[i: i+order])\n",
    "        if h in lm:\n",
    "            p = lm[h].get(w, 1/len(vocab))\n",
    "        else:\n",
    "            p = 1/len(vocab)\n",
    "        log_sum += math.log(p)\n",
    "    # compute perplexity\n",
    "    perplexity = math.exp(-log_sum/N)\n",
    "    return perplexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hVpItwZhI6ac"
   },
   "source": [
    "Let's evaluate the language model with different orders. You should see a decrease in perplexity as the order increases. As a reference, the perplexity of the unigram, bigram, trigram, and 4-gram language models should be around 795, 203, 141, and 130 respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "SpN70HA2H9C-",
    "outputId": "83a830db-0c66-4ffe-a7e6-d2587c7f6b8c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "order 1 ppl 794.5377104541699\n",
      "order 2 ppl 260.5186891747848\n",
      "order 3 ppl 260.54821138804743\n",
      "order 4 ppl 260.55785556146014\n"
     ]
    }
   ],
   "source": [
    "for o in [1, 2, 3, 4]:\n",
    "    lm = train_ngram_lm(data['train'], order=o)\n",
    "    print('order {} ppl {}'.format(o, compute_perplexity(lm, data['test'], vocab, order=o)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9tRCKZ5BJJOV"
   },
   "source": [
    "# Q2. Neural language models (70pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5jlXIjm6WZBE"
   },
   "source": [
    "In this part of the homework, we'll be using PyTorch to play around with neural language models. First, a quick warm up by implementing backpropagation within a *scalar* neural network. Then, you'll implement a neural language model using PyTorch's built-in modules.\n",
    "\n",
    "Firstly, run the cell below to import pytorch and set up the gradient checking functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "VOhQHHAPV6LD"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "device = torch.device('cpu')\n",
    "\n",
    "# checks equality between your gradients and those from autograd\n",
    "def gradient_check(params, your_gradient):\n",
    "    all_good = True\n",
    "    for key in params.keys():\n",
    "        if params[key].grad.size() != your_gradient[key].size():\n",
    "            print('GRADIENT ERROR for parameter %s, SIZE ERROR\\nyour size: %s\\nactual size: %s\\n'\\\n",
    "                % (key, your_gradient[key].size(), \n",
    "                   params[key].grad.size()))\n",
    "            all_good = False\n",
    "        elif not torch.allclose(params[key].grad, your_gradient[key], atol=1e-6):\n",
    "            print('GRADIENT ERROR for parameter %s, VALUE ERROR\\nyours: %s\\nactual: %s\\n'\\\n",
    "                % (key, your_gradient[key].detach(), \n",
    "                   params[key].grad))\n",
    "            all_good = False\n",
    "            \n",
    "    return all_good"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2kt0UNgpWuJ6"
   },
   "source": [
    "## Q2.1 Warm up with single neuron (10 pts)\n",
    "The following code cell trains a network with scalars (i.e., single neurons) in each layer on a small dataset of ten examples. All you have to do is translate the partial derivatives we computed into code. The network is defined as:\n",
    "\n",
    "<center>$\\text{h} = \\tanh(w_1 \\cdot \\text{input})$</center>\n",
    "\n",
    "<center>$\\text{pred} = \\tanh(w_2 \\cdot \\text{h})$</center>\n",
    "\n",
    "<center>$\\text{L} = 0.5 \\cdot (\\text{target} - \\text{pred})^2$</center>\n",
    "\n",
    "If you run the cell below, you should see \"GRADIENT ERRORS\". Once you implement the partial derivatives $\\frac{\\partial{L}}{\\partial{w_1}}$ and $\\frac{\\partial{L}}{\\partial{w_2}}$ correctly, you will instead see a \"SUCCESS\" message. **Do NOT modify any code outside of the block marked \"IMPLEMENT BACKPROP HERE\"!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HfLgwTNYWGt3",
    "outputId": "4f64e8ad-7366-4ad1-b280-f672fe4f8c27"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GRADIENT ERROR for parameter w1, VALUE ERROR\n",
      "yours: tensor([[-0.0013]])\n",
      "actual: tensor([[-0.0050]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# initialize model parameters\n",
    "params = {}\n",
    "params['w1'] = torch.randn(1, 1, requires_grad=True) # input > hidden with scalar weight w1\n",
    "params['w2'] = torch.randn(1, 1, requires_grad=True) # hidden > output with scalar weight w2\n",
    "\n",
    "# set up some training data\n",
    "inputs = torch.randn(20, 1)\n",
    "targets = inputs / 2\n",
    "\n",
    "# training loop\n",
    "all_good = True\n",
    "for i in range(len(inputs)):\n",
    "    \n",
    "    ## forward prop, then compute loss.\n",
    "    a = params['w1'] * inputs[i] # intermediate variable, following lecture notes\n",
    "    hidden = torch.tanh(a)\n",
    "    b = params['w2'] * hidden\n",
    "    pred = torch.tanh(b)\n",
    "    loss = 0.5 * (targets[i] - pred) ** 2 # compute square loss\n",
    "    loss.backward() # runs autograd\n",
    "    \n",
    "    \n",
    "    ####################\n",
    "    # TODO: IMPLEMENT BACKPROP HERE\n",
    "    # DO NOT MODIFY ANY CODE OUTSIDE OF THIS BLOCK!!!!\n",
    "    your_gradient = {}\n",
    "    your_gradient['w1'] = torch.zeros(params['w1'].size()) # implement dL/dw1\n",
    "    your_gradient['w2'] = torch.zeros(params['w2'].size()) # implement dL/dw2\n",
    "\n",
    "    # compute gradients\n",
    "    a = params['w1'] * inputs[i] # intermediate variable, following lecture notes\n",
    "    hidden = torch.tanh(a)\n",
    "    b = params['w2'] * hidden\n",
    "    pred = torch.tanh(b)\n",
    "\n",
    "    dL_db = (pred - targets[i]) * (1 - torch.tanh(b) ** 2)\n",
    "    dL_dh = dL_db * params['w2'].item() * (1 - torch.tanh(a) ** 2)\n",
    "\n",
    "    your_gradient['w2'] += dL_db * hidden\n",
    "    your_gradient['w1'] += dL_dh * inputs[i] * (1 - torch.tanh(a) ** 2)\n",
    "\n",
    "    # END \n",
    "    ####################\n",
    "    \n",
    "    if not gradient_check(params, your_gradient):\n",
    "        all_good = False\n",
    "        break\n",
    "    \n",
    "    # zero gradients after each training example\n",
    "    params['w1'].grad.zero_()\n",
    "    params['w2'].grad.zero_() \n",
    "    \n",
    "if all_good:\n",
    "    print('SUCCESS! you passed the gradient check.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c5bzxVSAOPUn"
   },
   "source": [
    "## Q2.2 RNN language model (20 pts)\n",
    "\n",
    "For this part of the homework, we will use **PyTorch** to build our model. The following code cell preprocesses the raw text so you can load it directly. The input to your model is a *minibatch* of sequences which takes the form of a  $N \\times L$ matrix  where $N$ is the batch size and $L$ is the maximum sequence length. For each minibatch, your models should produce an $N \\times L \\times V$ tensor where $V$ is the size of the vocabulary. This tensor stores the predicted probability distribution of the next word for every position of every sequence in the batch. Note that each batch is padded to dimensionality $L=40$ using the special padding token <*pad>*; similarly, each sequence begins with the <*bos>* token and ends with the <*eos>* token. Please look at the [PyTorch RNN documentation](https://pytorch.org/docs/stable/generated/torch.nn.RNN.html) if you're having problems getting started.\n",
    "\n",
    "First, run the following code cell to download the data.\n",
    "\n",
    "<font color=\"red\">Please change your Colab runtime to the GPU backend by going to \"Runtime > Change runtime type > Hardware accelerator > GPU\".</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xtOZWDtTSCAG",
    "outputId": "245ce512-43d5-4bef-ef4f-adc59bab5ce7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device:  cuda\n",
      "Wikitext data loaded!\n",
      "There are 28654 words in vocabulary\n",
      "Word id 0 stands for '<pad>'\n",
      "Word id 1 stands for '<unk>'\n",
      "Word id 2 stands for '<bos>'\n",
      "Word id 3 stands for '<eos>'\n",
      "Word id 4 stands for 'the'\n",
      "Word id 5 stands for ','\n",
      "Word id 6 stands for '.'\n",
      "Word id 7 stands for 'of'\n",
      "...\n",
      "tensor(1622368, device='cuda:0')\n",
      "Set up finished\n"
     ]
    }
   ],
   "source": [
    "import torch, pickle, os, sys, random, time\n",
    "from torch import nn, optim\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('device: ', device)\n",
    "\n",
    "# Load id2word from wikitext pickle\n",
    "with open('wikitext.pkl', 'rb') as f_in:\n",
    "    wikitext = pickle.load(f_in)\n",
    "\n",
    "wikitext['train'] = torch.LongTensor(wikitext['train']).to(device)\n",
    "wikitext['dev'] = torch.LongTensor(wikitext['valid']).to(device)\n",
    "wikitext['test'] = torch.LongTensor(wikitext['test']).to(device)\n",
    "idx_to_word = wikitext['id2word']\n",
    "word_to_idx = {idx_to_word[k]: k for k in idx_to_word}\n",
    "\n",
    "print(\"Wikitext data loaded!\")\n",
    "# Demonstrate id2word\n",
    "print('There are ' + str(len(idx_to_word)) + ' words in vocabulary')\n",
    "for id in range(8):\n",
    "    print('Word id ' + str(id) + \" stands for '\" + str(idx_to_word[id]) + \"\\'\")\n",
    "print('...')\n",
    "print((wikitext['train'] > 0).sum())\n",
    "    \n",
    "print('Set up finished')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x_3C-13G_m2Q"
   },
   "source": [
    "The following cell contains code for computing perplexity and training the neural language model. Run the cell, and please make sure you (at least roughly) understand what is happening, but **do not modify any part of it**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "onfjIrblON0g"
   },
   "outputs": [],
   "source": [
    "# function to evaluate LM perplexity on some input data, DO NOT MODIFY\n",
    "def compute_perplexity(dataset, net, bsz=64):\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=0, reduction='sum')\n",
    "    num_examples, seq_len = dataset.size()\n",
    "    \n",
    "    # we'll still use batches because we can't fit the whole\n",
    "    # validation set into GPU memory\n",
    "    batches = [(start, start + bsz) for start in range(0, num_examples, bsz)]\n",
    "    \n",
    "    total_unmasked_tokens = 0. # count how many unpadded tokens there are\n",
    "    nll = 0.\n",
    "    for b_idx, (start, end) in enumerate(batches):\n",
    "        batch = dataset[start:end]\n",
    "        ut = torch.nonzero(batch).size(0)\n",
    "        preds = net(batch)\n",
    "        targets = batch[:, 1:].contiguous().view(-1)\n",
    "        preds = preds[:, :-1, :].contiguous().view(-1, net.vocab_size)\n",
    "        loss = criterion(preds, targets)\n",
    "        nll += loss.detach()\n",
    "        total_unmasked_tokens += ut\n",
    "\n",
    "    perplexity = torch.exp(nll / total_unmasked_tokens).cpu()\n",
    "    return perplexity.data\n",
    "    \n",
    "\n",
    "# training loop for language models, DO NOT MODIFY!\n",
    "def train_lm(dataset, params, net):\n",
    "    \n",
    "    # since the first index corresponds to the PAD token, we just ignore it\n",
    "    # when computing the loss\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "    \n",
    "    optimizer = optim.Adam(net.parameters(), lr=params['learning_rate'])\n",
    "    num_examples, seq_len = dataset.size()    \n",
    "    batches = [(start, start + params['batch_size']) for start in\\\n",
    "               range(0, num_examples, params['batch_size'])]\n",
    "    \n",
    "    for epoch in range(params['epochs']):\n",
    "        ep_loss = 0.\n",
    "        start_time = time.time()\n",
    "        random.shuffle(batches)\n",
    "        net.train()\n",
    "        # for each batch, calculate loss and optimize model parameters            \n",
    "        for b_idx, (start, end) in enumerate(batches):\n",
    "            batch = dataset[start:end]\n",
    "            preds = net(batch)\n",
    "\n",
    "            preds = preds[:, :-1, :].contiguous().view(-1, net.vocab_size)\n",
    "            targets = batch[:, 1:].contiguous().view(-1)\n",
    "            loss = criterion(preds, targets)\n",
    "\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(net.parameters(), 3)\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            ep_loss += loss\n",
    "        \n",
    "        net.eval()\n",
    "        print('epoch: %d, loss: %0.2f, time: %0.2f sec, dev perplexity: %0.2f' %\\\n",
    "              (epoch, ep_loss, time.time()-start_time, compute_perplexity(wikitext['dev'], net)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PVwFC8cHLWye"
   },
   "source": [
    "Now implement the following class, which defines a recurrent neural language model, by implementing the `forward` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "IBC5pTTgIBVH"
   },
   "outputs": [],
   "source": [
    "class RNNLM(nn.Module):\n",
    "    def __init__(self, params):\n",
    "        super(RNNLM, self).__init__()\n",
    "        self.vocab_size = params['vocab_size']\n",
    "        self.d_emb = params['d_emb']  # size of word-embedding vector\n",
    "        self.d_hid = params['d_hid']  # vector size of the hidden layer\n",
    "        self.n_layer = 1\n",
    "        self.batch_size = params['batch_size']\n",
    "        \n",
    "        self.encoder = nn.Embedding(self.vocab_size, self.d_emb)\n",
    "        self.rnn = nn.RNN(self.d_emb, self.d_hid, self.n_layer, batch_first=True)\n",
    "        self.decoder = nn.Linear(self.d_hid, self.vocab_size)\n",
    "        \n",
    "    def forward(self, batch):\n",
    "        \"\"\"\n",
    "            IMPLEMENT ME!\n",
    "            Encode the data using the embedding layer you initialized.\n",
    "            Pass the encoded data and hidden states to your RNN.\n",
    "            Return unnormalized logits for each token's prediction.\n",
    "            \n",
    "            Why just logits? Check the document of torch.nn.CrossEntropyLoss,\n",
    "            since it combines nn.LogSoftmax() and nn.NLLLoss(), \n",
    "            you don't need to explicitly use the softmax function!\n",
    "        \"\"\"\n",
    "        batch_size, seq_len= batch.shape\n",
    "        hidden = (torch.zeros(self.n_layer, batch_size, self.d_hid).to(device))\n",
    "        x = self.encoder(batch)  # (batch_size, seq_len, d_emb)\n",
    "        x, hidden = self.rnn(x, hidden)\n",
    "        x = x.reshape(-1, self.d_hid) # (batch_size*seq_len, d_hid)\n",
    "        x = self.decoder(x)  # (batch_size*seq_len, vocab_size)\n",
    "        logits = x.reshape(batch_size, seq_len, self.vocab_size) # (batch_size, seq_len, vocab_size)\n",
    "        return logits   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LfzBU3SCWIso"
   },
   "source": [
    "Run the following cell to test that your implementation is at least returning tensors of the proper dimensionality. Note that this is just a sanity check. Your `RNNLM` might still be implemented incorrectly even if it passes. You will have to obtain a reasonable perplexity after training on WikiText to be certain that you've done it right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wv3NEVi7AaCE",
    "outputId": "83529cfc-a3d7-46f6-fd1b-e5573c500cc4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Congratulations, you passed the RNNLM test!\n"
     ]
    }
   ],
   "source": [
    "def test_RNNLM():\n",
    "    test_batch = torch.LongTensor(5, 4).random_(0, 10).to(device)\n",
    "    params = {}\n",
    "    params['vocab_size'] = len(idx_to_word)\n",
    "    params['d_emb'] = 8\n",
    "    params['d_hid'] = 8\n",
    "    params['batch_size'] = 5\n",
    "    testnet = RNNLM(params)\n",
    "    testnet.to(device)\n",
    "    test_output = testnet(test_batch)\n",
    "    assert test_output.shape[0] == params['batch_size'], \"size of dimension 0 is incorrect, expect %i but got %i\" % \\\n",
    "                                                          (params['batch_size'], test_output.shape[0])\n",
    "    assert test_output.shape[1] == test_batch.shape[1], \"size of dimension 1 is incorrect, expect %i but got %i\" % \\\n",
    "                                                          (test_batch.shape[1], test_output.shape[1])\n",
    "    assert test_output.shape[2] == params['vocab_size'], \"size of dimension 2 is incorrect, expect %i but got %i\" % \\\n",
    "                                                          (params['vocab_size'], test_output.shape[2])\n",
    "    print(\"Congratulations, you passed the RNNLM test!\")\n",
    "test_RNNLM()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7li14NHkLimS"
   },
   "source": [
    "Once you pass the above test, train your `RNNLM` model on WikiText by running the cell below. It should take a couple minutes per epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tGvKkmqMET1w",
    "outputId": "704e3d8d-b09a-4c65-fddf-8fc04e90e2fc",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, loss: 6398.67, time: 29.49 sec, dev perplexity: 182.50\n",
      "epoch: 1, loss: 5661.17, time: 29.16 sec, dev perplexity: 155.22\n",
      "epoch: 2, loss: 5319.08, time: 28.99 sec, dev perplexity: 148.12\n",
      "epoch: 3, loss: 5062.62, time: 29.03 sec, dev perplexity: 143.56\n",
      "epoch: 4, loss: 4854.10, time: 29.03 sec, dev perplexity: 146.26\n"
     ]
    }
   ],
   "source": [
    "# DO NOT CHANGE THESE HYPERPARAMETERS, WE WILL CHECK!\n",
    "params = {}\n",
    "params['vocab_size'] = len(idx_to_word)\n",
    "params['d_emb'] = 512\n",
    "params['d_hid'] = 256\n",
    "params['batch_size'] = 64\n",
    "params['epochs'] = 5\n",
    "params['learning_rate'] = 0.001\n",
    "\n",
    "RNNnet = RNNLM(params)\n",
    "RNNnet.to(device)\n",
    "train_lm(wikitext['train'], params, RNNnet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jwNvc39bLnqY"
   },
   "source": [
    "After training is finished, run the cell below to get the perplexity on the test set. If you did it right, your perplexity should be around 135-140."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xD6NYw62St2J",
    "outputId": "2c8c3113-92a8-4213-b819-c06367b4cd98"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test perplexity: 137.01\n"
     ]
    }
   ],
   "source": [
    "RNNnet.eval() # we're no longer training the network\n",
    "print('%s perplexity: %0.2f' % ('test', compute_perplexity(wikitext['test'], RNNnet)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lKu_fZJ6VUL8"
   },
   "source": [
    "## Q2.3 Neural Language Model with attention (30 pts)\n",
    "\n",
    "Only start working at this after you've correctly implemented the `RNNLM` in the previous problem, as you'll want to copy over some code here. \n",
    "Complete the foward function of both the `ATTNLM` and `Attention` modules by following the instructions in the comment block. **Each epoch may take 3-5 minutes to run, so start early!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "Ja339lSzVajG"
   },
   "outputs": [],
   "source": [
    "# An RNN language model with attention, you implement this!\n",
    "class ATTNLM(nn.Module):\n",
    "    def __init__(self, params):\n",
    "        super(ATTNLM, self).__init__()\n",
    "        \n",
    "        self.vocab_size = params['vocab_size']\n",
    "        self.d_emb = params['d_emb']\n",
    "        self.d_hid = params['d_hid']\n",
    "        self.n_layer = 1\n",
    "        self.btz = params['batch_size']\n",
    "        \n",
    "        self.encoder = nn.Embedding(self.vocab_size, self.d_emb)\n",
    "        self.attn = Attention(self.d_hid)\n",
    "        self.rnn = nn.RNN(self.d_emb, self.d_hid, self.n_layer, batch_first=True)\n",
    "        # the combined_W maps the combined hidden states and context vectors to d_hid \n",
    "        self.combined_W = nn.Linear(self.d_hid * 2, self.d_hid)\n",
    "        self.decoder = nn.Linear(self.d_hid, self.vocab_size)\n",
    "        \n",
    "\n",
    "    def forward(self, batch, return_attn_weights=False):\n",
    "        batch_size, seq_len= batch.shape\n",
    "        hidden = torch.zeros(self.n_layer, batch_size, self.d_hid).to(device)\n",
    "        encoded = self.encoder(batch)\n",
    "        \n",
    "        rnn_out, hidden = self.rnn(encoded, hidden)\n",
    "        \n",
    "        context_vectors, attn_weights = self.attn(rnn_out)\n",
    "        \n",
    "        combined = torch.cat((rnn_out, context_vectors), dim=2)\n",
    "        combined = self.combined_W(combined)\n",
    "        logits = self.decoder(combined.view(-1, self.d_hid))\n",
    "        logits = logits.view(batch_size, seq_len, -1)\n",
    "        \n",
    "        if return_attn_weights:\n",
    "            return attn_weights\n",
    "        \n",
    "        return logits\n",
    "        \n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, d_hidden):\n",
    "        super(Attention, self).__init__()\n",
    "        self.linear_w1 = nn.Linear(d_hidden, d_hidden)\n",
    "        self.linear_w2 = nn.Linear(d_hidden, 1)\n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        attn_weights = torch.zeros(batch_size, seq_len, seq_len).to(device)\n",
    "        \n",
    "        for t in range(seq_len):\n",
    "            attn_input = x[:, t, :].unsqueeze(1)\n",
    "            h_t = torch.tanh(self.linear_w1(x[:, :t+1, :]))\n",
    "            attn_scores = self.linear_w2(h_t).squeeze(2)\n",
    "            attn_dist = nn.functional.softmax(attn_scores, dim=1)\n",
    "            attn_weights[:, t, :t+1] = attn_dist\n",
    "            context_vector = torch.bmm(attn_dist.unsqueeze(1), x[:, :t+1, :]).squeeze(1)\n",
    "            if t == 0:\n",
    "                context_vectors = context_vector.unsqueeze(1)\n",
    "            else:\n",
    "                context_vectors = torch.cat((context_vectors, context_vector.unsqueeze(1)), dim=1)\n",
    "        \n",
    "        return context_vectors, attn_weights.tril()  # lower triangular"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nqyjOwGz8Y6b"
   },
   "source": [
    "Run the following cell to sanity check your implementation; do not continue until you pass all of the tests!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_0-MHHC2KYv8",
    "outputId": "8a543a58-58a9-4847-8264-f61d1245b8de"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Congratulations, you passed the ATTNLM test!\n"
     ]
    }
   ],
   "source": [
    "def test_ATTNLM():\n",
    "    test_batch = torch.LongTensor(5, 4).random_(0, 10).to(device)\n",
    "    params = {}\n",
    "    params['vocab_size'] = len(idx_to_word)\n",
    "    params['d_emb'] = 8\n",
    "    params['d_hid'] = 8\n",
    "    params['batch_size'] = 5\n",
    "    testnet = ATTNLM(params)\n",
    "    testnet.to(device)\n",
    "    test_output = testnet(test_batch)\n",
    "    assert test_output.shape[0] == params['batch_size'], \"size of dimension 0 is incorrect, expect %i but got %i\" % \\\n",
    "                                                          (params['batch_size'], test_output.shape[0])\n",
    "    assert test_output.shape[1] == test_batch.shape[1], \"size of dimension 1 is incorrect, expect %i but got %i\" % \\\n",
    "                                                          (test_batch.shape[1], test_output.shape[1])\n",
    "    assert test_output.shape[2] == params['vocab_size'], \"size of dimension 2 is incorrect, expect %i but got %i\" % \\\n",
    "                                                          (params['vocab_size'], test_output.shape[2])\n",
    "    testnet = ATTNLM(params)\n",
    "    testnet.to(device)\n",
    "    test_output = testnet(test_batch, return_attn_weights=True)\n",
    "    assert test_output.shape[0] == params['batch_size'], \"size of dimension 0 is incorrect, expect %i but got %i\" % \\\n",
    "                                                          (params['batch_size'], test_output.shape[0])\n",
    "    assert test_output.shape[1] == test_batch.shape[1], \"size of dimension 1 is incorrect, expect %i but got %i\" % \\\n",
    "                                                          (test_batch.shape[1], test_output.shape[1])\n",
    "    assert test_output.shape[2] == test_batch.shape[1], \"size of dimension 2 is incorrect, expect %i but got %i\" % \\\n",
    "                                                          (test_batch.shape[1], test_output.shape[2])\n",
    "    prob_dist = torch.sum(test_output, dim=2)[:, 1:]\n",
    "    assert all([x > 0.99 and x < 1.01 for x in prob_dist.reshape(-1)]), \"attention weights not properly normalized, got {}\".format(prob_dist)\n",
    "    print(\"Congratulations, you passed the ATTNLM test!\")\n",
    "\n",
    "test_ATTNLM()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vRxeoJW3Ltzk"
   },
   "source": [
    "Now, train your `ATTNLM` model on WikiText by running the following code cell. If the perplexity on dev set is `nan` or `inf`, it is likely the model is corrupted due to gradient exploding/vanishing or other numerical instability issue; stop this cell and run it again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "81LKL_7pKYAC",
    "outputId": "62b43fab-6cce-4716-e8ea-ed3d006277e7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, loss: 6458.55, time: 67.59 sec, dev perplexity: 195.64\n",
      "epoch: 1, loss: 5833.66, time: 67.23 sec, dev perplexity: 169.06\n",
      "epoch: 2, loss: 5552.28, time: 66.82 sec, dev perplexity: 156.35\n",
      "epoch: 3, loss: 5330.04, time: 67.21 sec, dev perplexity: 148.09\n",
      "epoch: 4, loss: 5141.65, time: 66.84 sec, dev perplexity: 147.16\n",
      "epoch: 5, loss: 4978.49, time: 67.66 sec, dev perplexity: 144.76\n"
     ]
    }
   ],
   "source": [
    "# DO NOT CHANGE THESE HYPERPARAMETERS, WE WILL CHECK!\n",
    "params = {}\n",
    "params['vocab_size'] = len(idx_to_word)\n",
    "params['d_emb'] = 512\n",
    "params['d_hid'] = 256\n",
    "params['n_layer'] = 1\n",
    "params['batch_size'] = 64\n",
    "params['epochs'] = 6\n",
    "params['learning_rate'] = 0.0005\n",
    "\n",
    "ATTNnet = ATTNLM(params)\n",
    "ATTNnet.cuda()\n",
    "train_lm(wikitext['train'], params, ATTNnet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ENa03ZlJLyFF"
   },
   "source": [
    "Finally, compute the perplexity on the test set. If you implemented it correctly, you should get a perplexity of around 145-150. Due to random effects, it is possible to get perplexity slightly lower than 145. Make sure you didn't add any additional nonlinearity operation which can lead to lower perplexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LoOvc6quW0Ef",
    "outputId": "9165c1ac-5da6-44d8-f04e-bc4c2922637a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test perplexity: 135.30\n"
     ]
    }
   ],
   "source": [
    "ATTNnet.eval() # we're no longer training the network\n",
    "print('%s perplexity: %0.2f' % ('test', compute_perplexity(wikitext['test'], ATTNnet)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TCyBN6KtEpLy"
   },
   "source": [
    "## Q2.4 Generate text from the neural LMs (5 pts)\n",
    "Run the following code cell to generate some text from your `RNNLM` and `ATTNLM`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fkzzhEYCCY5A",
    "outputId": "6c9b08db-bab1-4f5d-ccdd-9c0e0508c564"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rnn completion:  he is the virginia image of 24  .\n"
     ]
    }
   ],
   "source": [
    "def sample_from_lm(net, context, max_words=50):\n",
    "  \n",
    "    with torch.no_grad():\n",
    "        for i in range(max_words):\n",
    "            data = torch.LongTensor([context]).to(device)\n",
    "            decoded = net(data)\n",
    "            decoded = decoded[0, -1].exp().cpu()\n",
    "            w_i = torch.multinomial(decoded, 1)[0].item()\n",
    "            if w_i in [1, 2, 3]:\n",
    "                continue\n",
    "            context.append(w_i)\n",
    "\n",
    "        return context\n",
    "\n",
    "word_to_idx = dict((v,k) for (k,v) in idx_to_word.items())\n",
    "context = [word_to_idx[w] for w in 'he is the '.split()]\n",
    "\n",
    "rnn_completion = sample_from_lm(RNNnet, context)\n",
    "print('rnn completion: ', ' '.join([idx_to_word[w] for w in rnn_completion]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QYg9ki0dYoyi",
    "outputId": "1fdd8549-048d-4260-ed2e-290c2c463139"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attention rnn completion:  he is the inconsistencies telecommunications koopa ruwan bone marrow transplant ahk ibarra .\n"
     ]
    }
   ],
   "source": [
    "word_to_idx = dict((v,k) for (k,v) in idx_to_word.items())\n",
    "context = [word_to_idx[w] for w in 'he is the '.split()]\n",
    "\n",
    "rnn_completion = sample_from_lm(ATTNnet, context)\n",
    "print('attention rnn completion: ', ' '.join([idx_to_word[w] for w in rnn_completion]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KU63vLoNE76l"
   },
   "source": [
    "Do you notice any differences in coherence or grammaticality compared to the n-gram models? What about any differences between the `RNNLM` and the `ATTNLM`? If you observed any distinct differences, explain why you think they exist; if not, explain why all of the outputs appear to be of similar quality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6Krumh-KN3fw"
   },
   "source": [
    "### <font color=\"red\">*Answer in two to four sentences here*.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As compared to the n-gram models, the neural language models (RNNLM and ATTNLM) have demonstrated better coherence and grammaticality. In addition, ATTNLM has outperformed RNNLM in terms of generating more coherent and grammatically correct sentences, as it utilizes attention to focus on different parts of the input sequence. This makes the model more effective in modeling long-range dependencies and capturing context-specific information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xBGJPkitfizQ"
   },
   "source": [
    "## Q2.5 Interpreting attention (5 pts)\n",
    "Finally, let's visualize some attention heatmaps by running the following two code cells. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 297
    },
    "id": "jXykMRedFfE2",
    "outputId": "0713e506-e5e3-46b9-ef75-b0c55fb984fd"
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "def plot_attn_heatmap(sent):\n",
    "  \n",
    "    sent_in_id = [word_to_idx[w] for w in sent.split()]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        data = torch.LongTensor([sent_in_id]).to(device)\n",
    "        weights = ATTNnet(data, return_attn_weights=True)\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    sent_sp = sent.split()\n",
    "    ax.set_xticks(np.arange(len(sent_sp)))\n",
    "    ax.set_yticks(np.arange(len(sent_sp)))\n",
    "    ax.set_xticklabels(sent_sp)\n",
    "    ax.set_yticklabels(sent_sp)\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha='right', rotation_mode=\"anchor\")\n",
    "\n",
    "    plt.imshow(weights[0, :].cpu())\n",
    "\n",
    "sent = \"top warning signs earth is warming , according to experts\"\n",
    "plot_attn_heatmap(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 290
    },
    "id": "szDBrOdpFflZ",
    "outputId": "96f09d9e-10d9-4598-f298-7ceb336b7a3c"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'word_to_idx' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_5884\\1065687754.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0msent\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"us cities lose 36 million trees each year . here is why it matters \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mplot_attn_heatmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_5884\\1895593329.py\u001b[0m in \u001b[0;36mplot_attn_heatmap\u001b[1;34m(sent)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mplot_attn_heatmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0msent_in_id\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mword_to_idx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_5884\\1895593329.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mplot_attn_heatmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0msent_in_id\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mword_to_idx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'word_to_idx' is not defined"
     ]
    }
   ],
   "source": [
    "sent = \"us cities lose 36 million trees each year . here is why it matters \"\n",
    "plot_attn_heatmap(sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FGh2jq9tG50n"
   },
   "source": [
    "Each row of these plots represents the attention weights on the history tokens when the model is trying to predict the next word. For example, the third row of the first plot can be interpreted as the attention weights over \"top\" and \"warning\" when predicting \"signs\"; you'll note that the rest of the row is black (i.e., zero attention on future words). Are these attention maps interpretable? If you (as a human) were solving the same word prediction problem, would you focus on the same words as the ATTNLM does?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DBfgLYgNN3fx"
   },
   "source": [
    "### <font color=\"red\">*Answer in two to four sentences here*.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The attention maps can be somewhat interpretable as they show which previous words the model is focusing on while predicting the next word. However, the maps can be complex and difficult to interpret, especially as the sequences become longer. As a human, I might not focus on the same words as the ATTNLM does since the model might be using certain patterns or associations in the training data that are not obvious or intuitive to humans."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "machine_shape": "hm",
   "provenance": [],
   "toc_visible": true
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
