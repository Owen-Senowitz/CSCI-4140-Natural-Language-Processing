{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4139cc19-dd23-4ffa-b6a2-a1b70a6daec6",
   "metadata": {},
   "source": [
    "<center><h1>CSCI 4140: Natural Language Processing</h1></center>\n",
    "<center><h1>CSCI/DASC 6040: Computational Analysis of Natural Languages</h1></center>\n",
    "\n",
    "<center><h6>Spring 2025</h6></center>\n",
    "<center><h6>Homework 1 - N-gram models</h6></center>\n",
    "<center><h6>Due Sunday, January 26, at 11:59 PM</h6></center>\n",
    "\n",
    "<center><font color='red'>Do not redistribute without the instructor’s written permission.</font></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "166d5179-89ef-4186-9af3-ce63ff95cc8b",
   "metadata": {},
   "source": [
    "The learning goals of this assignment are to:\n",
    "\n",
    "- Understand how to compute language model probabilities using maximum likelihood estimation.\n",
    "- Implement back-off.\n",
    "- Have fun using a language model to probabilistically generate texts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40408d08-1f30-4286-be61-ac741db46121",
   "metadata": {},
   "source": [
    "# N-gram Language model\n",
    "- For undergraduates: **100 pts + 10 extra credit pts**\n",
    "- For graduates: **110 pts**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79f41ccf-d54b-4f5d-bd2d-4c8811cd84d1",
   "metadata": {},
   "source": [
    "## Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "84910e85-68eb-41ab-996f-65cad5d8071e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import *\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b02bac8f-9980-4905-8029-1d48434008cd",
   "metadata": {},
   "source": [
    "We'll start by loading the data. The [WikiText language modeling dataset](https://huggingface.co/datasets/Salesforce/wikitext) is a collection of tokens extracted from the set of verified Good and Featured articles on Wikipedia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "22459dd5-ddf4-4e53-9e4a-570646a15820",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {'test': '', 'train': '', 'valid': ''}\n",
    "\n",
    "for data_split in data:\n",
    "    fname = \"wiki.{}.tokens\".format(data_split)\n",
    "    with open(fname, 'r') as f_wiki:\n",
    "        data[data_split] = f_wiki.read().lower().split()\n",
    "\n",
    "vocab = list(set(data['train'] + ['<unk>']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38de2aac-d6c2-4ec2-b0d1-5e2305949705",
   "metadata": {},
   "source": [
    "Now have a look at the data by running this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "d733449a-e810-4cd1-a03d-e309d41e1dd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train : ['=', 'valkyria', 'chronicles', 'iii', '=', 'senjō', 'no', 'valkyria', '3', ':'] ...\n",
      "dev : ['=', 'homarus', 'gammarus', '=', 'homarus', 'gammarus', ',', 'known', 'as', 'the'] ...\n",
      "test : ['=', 'robert', '<unk>', '=', 'robert', '<unk>', 'is', 'an', 'english', 'film'] ...\n",
      "first 10 words in vocab: ['toilet', 'millennia', 'enclosed', 'blitt', 'kishon', 'explosions', 'nadal', 'acquire', 'redesign', 'puzzle']\n"
     ]
    }
   ],
   "source": [
    "print('train : %s ...' % data['train'][:10])\n",
    "print('dev : %s ...' % data['valid'][:10])\n",
    "print('test : %s ...' % data['test'][:10])\n",
    "print('first 10 words in vocab: %s' % vocab[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "872e25cc-9b86-408a-b6f0-1fb139effc34",
   "metadata": {},
   "source": [
    "## Q1: Train N-gram language model (60 pts)\n",
    "\n",
    "Complete the following `train_ngram_lm` function based on the following input/output specifications. If you've done it right, you should pass the tests in the cell below.\n",
    "\n",
    "*Input:*\n",
    "+ **data**: the data object created in the cell above that holds the tokenized Wikitext data\n",
    "+ **order**: the order of the model (i.e., the \"n\" in \"n-gram\" model). If order=3, we compute $p(w_2 | w_0, w_1)$.\n",
    "\n",
    "*Output:*\n",
    "+ **lm**: A dictionary where the key is the history and the value is a probability distribution over the next word computed using the maximum likelihood estimate from the training data. Importantly, this dictionary should include *backoff* probabilities as well; e.g., for order=4, we want to store $p(w_3 | w_0,w_1,w_2)$ as well as $p(w_3|w_1,w_2)$ and $p(w_3|w_2)$. \n",
    "\n",
    "Each key should be a single string where the words that form the history have been concatenated using spaces. Given a key, its corresponding value should be a dictionary where each word type in the vocabulary is associated with its probability of appearing after the key. For example, the entry for the history 'w1 w2' should look like:\n",
    "\n",
    "    \n",
    "    lm['w1 w2'] = {'w0': 0.001, 'w1' : 1e-6, 'w2' : 1e-6, 'w3': 0.003, ...}\n",
    "    \n",
    "In this example, we also want to store `lm['w2']` and `lm['']`, which contain the bigram and unigram distributions respectively.\n",
    "\n",
    "*Hint:* You might find the **defaultdict** and **Counter** classes in the **collections** module to be helpful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "19adcd85-9e8e-4aae-b32b-05029a953dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ngram_lm(data, order=3):\n",
    "    \"\"\"\n",
    "        Train n-gram language model\n",
    "    \"\"\"\n",
    "    \n",
    "    # pad (order-1) special tokens to the left\n",
    "    # for the first token in the text\n",
    "    order -= 1\n",
    "    data = ['<S>'] * (order - 1) + data\n",
    "    lm = defaultdict(Counter)\n",
    "    for i in range(len(data) - order):\n",
    "        for n in range(1, order + 2):\n",
    "            history = ' '.join(data[i:i + n - 1])\n",
    "            word = data[i + n - 1]\n",
    "            lm[history][word] += 1\n",
    "    \n",
    "    for history in lm:\n",
    "        total_count = sum(lm[history].values())\n",
    "        for word in lm[history]:\n",
    "            lm[history][word] /= total_count\n",
    "\n",
    "    return lm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "a035c635-bddb-480b-940a-fc38a0482b0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checking empty history ...\n",
      "checking probability distributions ...\n",
      "checking lengths of histories ...\n",
      "checking word distribution values ...\n",
      "Congratulations, you passed the ngram check!\n"
     ]
    }
   ],
   "source": [
    "def test_ngram_lm():\n",
    "  \n",
    "    print('checking empty history ...')\n",
    "    lm1 = train_ngram_lm(data['train'], order=1)\n",
    "    assert '' in lm1, \"empty history should be in the language model!\"\n",
    "    \n",
    "    print('checking probability distributions ...')\n",
    "    lm2 = train_ngram_lm(data['train'], order=2)\n",
    "    sample = [sum(lm2[k].values()) for k in random.sample(list(lm2), 10)]\n",
    "    assert all([a > 0.999 and a < 1.001 for a in sample]), \"lm[history][word] should sum to 1!\"\n",
    "    \n",
    "    print('checking lengths of histories ...')\n",
    "    lm3 = train_ngram_lm(data['train'], order=3)\n",
    "    assert len(set([len(k.split()) for k in list(lm3)])) == 3, \"lm object should store histories of all sizes!\"\n",
    "    \n",
    "    print('checking word distribution values ...')\n",
    "    assert lm1['']['the'] < 0.064 and lm1['']['the'] > 0.062 and \\\n",
    "           lm2['the']['first'] < 0.017 and lm2['the']['first'] > 0.016 and \\\n",
    "           lm3['the first']['time'] < 0.106 and lm3['the first']['time'] > 0.105, \\\n",
    "           \"values do not match!\"\n",
    "    \n",
    "    print(\"Congratulations, you passed the ngram check!\")\n",
    "    \n",
    "test_ngram_lm()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c298d5-e036-4a1e-953c-a3888336b8bc",
   "metadata": {},
   "source": [
    "## Q2: Generate text from n-gram language model (40 pts)\n",
    "\n",
    "Complete the following `generate_text` function based on these input/output requirements:\n",
    "\n",
    "*Input:*\n",
    "\n",
    "+ **lm**: the lm object is the dictionary you return from  the **train_ngram_lm** function\n",
    "+ **vocab**: vocab is a list of unique word types in the training set, already computed for you during data loading.\n",
    "+ **context**: the input context string that you want to condition your language model on, should be a space-separated string of tokens\n",
    "+ **order**: order of your language model (i.e., \"n\" in the \"n-gram\" model)\n",
    "+ **num_tok**: number of tokens to be generated following the input context\n",
    "\n",
    "\n",
    "*Output:*\n",
    "\n",
    "+ generated text, should be a space-separated string\n",
    "    \n",
    "*Hint:*\n",
    "\n",
    "After getting the next-word distribution given history, try using **[numpy.random.choice](https://numpy.org/doc/stable/reference/random/generated/numpy.random.choice.html)** to sample the next word from the distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "9833b768-b01c-4f4e-9bca-70019f18b16d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate text\n",
    "def generate_text(lm, vocab, context=\"he is the\", order=3, num_tok=25):\n",
    "    \n",
    "    # The goal is to generate new words following the context\n",
    "    # If context has more tokens than the order of lm, \n",
    "    # generate text that follows the last (order-1) tokens of the context\n",
    "    # and store it in the variable `history`\n",
    "    order -= 1\n",
    "    history = context.split()[-order:]\n",
    "    # `out` is the list of tokens of context\n",
    "    # you need to append the generated tokens to this list\n",
    "    out = context.split()\n",
    "        \n",
    "    for i in range(num_tok):\n",
    "        history_str = ' '.join(history)\n",
    "        while history_str not in lm and history_str:\n",
    "            history = history[1:]\n",
    "            history_str = ' '.join(history)\n",
    "        dist = lm[history_str] if history_str in lm else {word: 1 / len(vocab) for word in vocab}\n",
    "        next_word = np.random.choice(list(dist.keys()), p=list(dist.values()))\n",
    "        out.append(next_word)\n",
    "        history = (history + [next_word])[-order:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4fbab80-83be-4789-833e-ad77eb8b06d5",
   "metadata": {},
   "source": [
    "Now try to generate some texts, generated by ngram language model with different orders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "74e4f974-09f0-4fea-9067-7dba785dc0e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "order = 1\n",
    "generate_text(train_ngram_lm(data['train'], order=order), vocab, context='he is the', order=order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "a21de76f-4864-439d-97a1-d78d250d7358",
   "metadata": {},
   "outputs": [],
   "source": [
    "order = 2\n",
    "generate_text(train_ngram_lm(data['train'], order=order), vocab, context='he is the', order=order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "0ac1484f-00bb-4519-9134-5b447b68b2b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "order = 3\n",
    "generate_text(train_ngram_lm(data['train'], order=order), vocab, context='he is the', order=order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "237ee800-b881-4173-8c95-525213b6136f",
   "metadata": {},
   "outputs": [],
   "source": [
    "order = 4\n",
    "generate_text(train_ngram_lm(data['train'], order=order), vocab, context='he is the', order=order)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe32855-37be-494e-bb66-b96bdc6b5f5d",
   "metadata": {},
   "source": [
    "## Q3 : Evaluate the models (10 pts)\n",
    "Now let's evaluate the models quantitively using the intrinsic metric **perplexity**. \n",
    "\n",
    "Recall perplexity is the inverse probability of the test text\n",
    "$$\\text{PP}(w_1, \\dots, w_t) = P(w_1, \\dots, w_t)^{-\\frac{1}{T}}$$\n",
    "\n",
    "For an n-gram model, perplexity is computed by\n",
    "$$\\text{PP}(w_1, \\dots, w_t) = \\left[\\prod_{t=1}^T P(w_t|w_{t-1},\\ldots,w_{t-n+1})\\right]^{-\\frac{1}{T}}$$\n",
    "\n",
    "To address the numerical issue (underflow), we usually compute\n",
    "$$\\text{PP}(w_1, \\dots, w_t) = \\exp\\left(-\\frac{1}{T}\\sum_i \\log P(w_t|w_{t-1},\\ldots,w_{t-n+1})\\right)$$\n",
    "\n",
    "\n",
    "*Input:*\n",
    "\n",
    "+ **lm**: the language model you trained (the object you returned from the `train_ngram_lm` function)\n",
    "+ **data**: test data\n",
    "+ **vocab**: the list of unique word types in the training set\n",
    "+ **order**: order of the lm\n",
    "\n",
    "*Output:*\n",
    "\n",
    "+ the perplexity of test data\n",
    "\n",
    "*Hint:*\n",
    "\n",
    "+ If the history is not in the **lm** object, back-off to (n-1) order history to check if it is in **lm**. If no history can be found, just use `1/|V|` where `|V|` is the size of vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "87e412a7-2ad8-4a79-a4b5-31bc81295e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import log, exp\n",
    "def compute_perplexity(lm, data, vocab, order=3):\n",
    "    \"\"\"\n",
    "    Compute the perplexity of the language model on test data\n",
    "    \"\"\"\n",
    "    order -= 1\n",
    "    data = ['<S>'] * order + data\n",
    "    log_sum = 0\n",
    "    for i in range(len(data) - order):\n",
    "        h, w = ' '.join(data[i: i+order]), data[i+order]\n",
    "        while h not in lm and h:\n",
    "            h = ' '.join(h.split()[1:])\n",
    "        prob = lm[h][w] if h in lm and w in lm[h] else 1 / len(vocab)\n",
    "        log_sum += log(prob)\n",
    "    \n",
    "    T = len(data) - order\n",
    "    perplexity = exp(-log_sum / T)\n",
    "    return perplexity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02749daa-96b5-4f50-aa1c-0279a6e24cef",
   "metadata": {},
   "source": [
    "Let's evaluate the language model with different orders. You should see a decrease in perplexity as the order increases. As a reference, the perplexity of the unigram, bigram, trigram, and 4-gram language models should be around 795, 203, 141, and 130 respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "f5293167-78a8-49c6-93dd-cbfdff8c3212",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "order 1 ppl 794.5377104541699\n",
      "order 2 ppl 260.52326835178815\n",
      "order 3 ppl 558.3907492614328\n",
      "order 4 ppl 942.759854189961\n"
     ]
    }
   ],
   "source": [
    "for o in [1, 2, 3, 4]:\n",
    "    lm = train_ngram_lm(data['train'], order=o)\n",
    "    print('order {} ppl {}'.format(o, compute_perplexity(lm, data['test'], vocab, order=o)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu_ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
